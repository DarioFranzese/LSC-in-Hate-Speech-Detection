Name;Link;Size;Labels;Timespan;Note
DigHist (early modern english);https://github.com/SanneHoeken/DigHist/tree/main;~10k;LSC + Hatefulness;1485-1603;"The labels refer to a small part of the dataset since a huge part is used to build the ""hate dimension""."
Toraman22;https://github.com/metunlp/hate-speech?utm_source=chatgpt.com;68/60k;Hate(2%), Offensive(19%), Normal(79%);2006-2021;Dataset has to be cleaned from turkish sentences. V2 dataset is a subset of V1 containing only the tweets with agreement > 80%.
MultiTarget;https://zenodo.org/records/10812805;~11k;Multilable (target of discrimination);2020-2022;"""biased"" means the tweet is biased towards a social class, ""calling out"" means they're calling out bias. I've studied the cases where these values are true, although it doesn't mean they are hate speech instances (we might need to classificate them)."
#MeToo;https://www.kaggle.com/datasets/rahulgoel1106/hatred-on-twitter-during-metoo-movement;~800k (11%);Binary;Sep 2018 - Feb 2019;
ExtremeBB;https://arxiv.org/pdf/2111.04479;56M;Solo Hate;2001-2023 (don't know if timestamped);Needs access, the paper is very well done, includings some statistics on data. I couldn't see the actuaal data tho.
Hate Speech tweets;https://www.kaggle.com/datasets/subhajournal/normal-hate-and-offensive-speeches?resource=download;~3k;Normal, Offensive, Hate;feb-23;There are some outliers in the dataset that are not declared even in the kaggle page (which is weird)
Civil Comments;https://www.tensorflow.org/datasets/catalog/civil_comments;1.8M (5.9%);Hatefulness (toxicity);2015-2017;"Dataset used for a kaggle task, in this classification task examples with ""toxicity"" value above 0.5 were labelled as hateful, so i did the same, we could work on that tho."
Online games harrassment;https://ub-web.de/research/;34k(137+207 harmful);;2011-2015;SQL dump given the annotation is weird and does not match what is said on the website(the number does not match).
2020 US elections;https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/stance-hof/;2700;Binary;2020 (non timestamped);Specific for Biden v Trump 2020
White Supremacy;https://aclanthology.org/W18-5102.pdf;~10k (11.29% hate);Binary;2002-2017 (non timestamped);Read REDME for info about filename structure. Each sentence has its own file.
Syntetic Dataset ;https://www.kaggle.com/datasets/ziya07/hate-speech-detection-dataset-for-social-media;1829;Normal, Offensive, Hate;2022-2024;This is NOT collected data, it has been generated. It might be helpful at some point but it doesn't look anything special (very generic slurs).
Multi-Event Tweet Dataset;https://arxiv.org/pdf/2210.05401;  ;;;Event based dataset for Misinformation. Lack of timestamps (although range is given) and not-hate speech labelled.
Gab Hate Corpus (GHC);https://osf.io/edua3/;27,665;Multilabel;Jan 2018 - Oct 2018 (non timestamped);Manually multi-labelled based on hate rethoric, not so easy to use.
MetaHate;https://huggingface.co/datasets/irlab-udc/metahate;1,1M (28% hate);Binary;;Since it's not diachronic it might just be useful if we want to try to generate new examples or we might want to extract target words istances.
TempoWiC;https://github.com/cardiffnlp/TempoWiC;~12k;Binary sense change (WiC style);Jan 2019 - Sep 2021;Dataset for LSC task, every row has two tweets in two different times sharing one same target word. These are not hate based (maybe some of them).
SemEval-2020 Task 1 (LSC);https://www.ims.uni-stuttgart.de/en/research/resources/corpora/sem-eval-ulscd/;~250k, ~350k;Both Binary and Graded change;1810-1860 , 1910-1960 (non timestamped);Large scale multilingual dataset for LSC task, as above, not hate based.
Gab Hate Corpus (GHC);https://osf.io/edua3/;27,665;Multilabel;Jan 2018 - Oct 2018 (non timestamped);Manually multi-labelled based on hate rethoric, not so easy to use.
