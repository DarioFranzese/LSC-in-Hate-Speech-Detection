{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9dc7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "\n",
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\lexicon.json', 'r') as f:\n",
    "    lexicon = json.load(f)\n",
    "    lexicon = set(w['word'].lower() for w in lexicon)\n",
    "\n",
    "counts = {word: 0 for word in lexicon}\n",
    "\n",
    "pattern = re.compile(\n",
    "    r'\\b(' + '|'.join(re.escape(word) for word in lexicon) + r')\\b',\n",
    "    re.IGNORECASE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbb97c",
   "metadata": {},
   "source": [
    "CivilComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e51a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CivilComments/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50575342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999516/1999516 [46:46<00:00, 712.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['comment_text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254c64d",
   "metadata": {},
   "source": [
    "MeToo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f4aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "df = pd.read_csv(\"MeToo/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74fd197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 807174/807174 [07:56<00:00, 1692.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f91e8",
   "metadata": {},
   "source": [
    "HS Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bc6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_classes = [\"Hate\", \"Normal\", \"Offensive\"]\n",
    "hst_dfs = []\n",
    "\n",
    "for cls in hst_classes:\n",
    "    for i in range(1, 5):  # files 1 to 4\n",
    "        file_name = f\"HS Tweets/{cls}_Speeches_{i}.csv\"\n",
    "        tmp = pd.read_csv(file_name)\n",
    "        tmp[\"class\"] = cls\n",
    "        hst_dfs.append(tmp)\n",
    "\n",
    "# Combine all files into a single DataFrame\n",
    "df = pd.concat(hst_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b018e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3061/3061 [00:03<00:00, 998.79it/s] \n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['full_text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a05c1",
   "metadata": {},
   "source": [
    "MultiTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9039ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MultiTarget/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2184d3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11243/11243 [00:12<00:00, 932.81it/s] \n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['Text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e69c5",
   "metadata": {},
   "source": [
    "Toraman22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3fcc784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: DtypeWarning: Columns (0,4,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Toraman22/v2.tsv\", sep=\"\\t\")\n",
    "\n",
    "# English Only\n",
    "df = df[df[\"language\"] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dfb1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102066/102066 [00:58<00:00, 1738.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "\n",
    "        for word in matches:\n",
    "            counts[word.lower()] +=1\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04875e08",
   "metadata": {},
   "source": [
    "GHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.read_csv('GHC/ghc_train.tsv', sep='\\t')\n",
    "df2 = pd.read_csv('GHC/ghc_test.tsv', sep='\\t')\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46b6a47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27546/27546 [00:15<00:00, 1770.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ab7bbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "were: 215841\n",
      "long: 82852\n",
      "political: 65041\n",
      "little: 63289\n",
      "family: 53310\n",
      "stuff: 48272\n",
      "hard: 45687\n",
      "deal: 34618\n",
      "pretty: 33133\n",
      "game: 32043\n",
      "face: 30953\n",
      "rape: 29590\n",
      "illegal: 29407\n",
      "stupid: 28562\n",
      "head: 26886\n",
      "behind: 25443\n",
      "worse: 23660\n",
      "fire: 22556\n",
      "moment: 22496\n",
      "easy: 20038\n",
      "trade: 19750\n",
      "pass: 19667\n",
      "funny: 19455\n",
      "piece: 17756\n",
      "hell: 17598\n",
      "african: 16086\n",
      "murder: 14971\n",
      "evil: 14833\n",
      "break: 14446\n",
      "waste: 13558\n",
      "blood: 13458\n",
      "ignorant: 11278\n",
      "cold: 11105\n",
      "beat: 10480\n",
      "fast: 10138\n",
      "estate: 9889\n",
      "negative: 9684\n",
      "idiot: 9616\n",
      "tired: 9490\n",
      "brown: 8999\n",
      "daughter: 8955\n",
      "trouble: 8911\n",
      "cheap: 8910\n",
      "dumb: 8493\n",
      "mark: 8276\n",
      "fish: 8275\n",
      "hide: 7998\n",
      "silly: 7765\n",
      "nazi: 7671\n",
      "fool: 7531\n",
      "attorney: 7528\n",
      "crap: 7436\n",
      "people's: 7196\n",
      "jewish: 7030\n",
      "broke: 6877\n",
      "trash: 6828\n",
      "wide: 6827\n",
      "garbage: 6703\n",
      "ball: 6668\n",
      "drunk: 6643\n",
      "busy: 6524\n",
      "slow: 6423\n",
      "dirty: 6363\n",
      "dude: 6312\n",
      "animal: 6256\n",
      "rock: 6192\n",
      "weapon: 6026\n",
      "wild: 6011\n",
      "danger: 5973\n",
      "bike: 5938\n",
      "skin: 5885\n",
      "ugly: 5520\n",
      "cant: 5477\n",
      "useless: 5378\n",
      "suit: 5370\n",
      "hole: 5336\n",
      "stuck: 5329\n",
      "faced: 5269\n",
      "troll: 5268\n",
      "camp: 5159\n",
      "turkey: 5133\n",
      "bitch: 5077\n",
      "charter: 5034\n",
      "professor: 5021\n",
      "tool: 4987\n",
      "blow: 4969\n",
      "insane: 4912\n",
      "loser: 4743\n",
      "nasty: 4683\n",
      "bully: 4595\n",
      "heat: 4541\n",
      "loud: 4527\n",
      "firm: 4435\n",
      "blah: 4408\n",
      "hang: 4143\n",
      "loose: 4063\n",
      "killer: 4036\n",
      "queen: 3934\n",
      "those people: 3918\n",
      "finger: 3832\n",
      "bubble: 3809\n",
      "terror: 3808\n",
      "rush: 3777\n",
      "screaming: 3612\n",
      "jack: 3535\n",
      "witch: 3525\n",
      "reject: 3513\n",
      "queer: 3478\n",
      "aggressive: 3477\n",
      "bull: 3457\n",
      "load: 3449\n",
      "cruel: 3381\n",
      "suck: 3359\n",
      "vice: 3262\n",
      "drain: 3262\n",
      "band: 3262\n",
      "root: 3250\n",
      "dick: 3175\n",
      "screw: 3101\n",
      "pack: 3051\n",
      "jerk: 3025\n",
      "frank: 2981\n",
      "rage: 2967\n",
      "chicken: 2917\n",
      "lame: 2842\n",
      "moron: 2830\n",
      "devil: 2824\n",
      "user: 2812\n",
      "tear: 2804\n",
      "cult: 2792\n",
      "rude: 2726\n",
      "moose: 2618\n",
      "petty: 2604\n",
      "stretch: 2574\n",
      "wood: 2557\n",
      "flip: 2483\n",
      "liability: 2474\n",
      "you people: 2436\n",
      "communism: 2401\n",
      "precious: 2335\n",
      "junk: 2332\n",
      "buck: 2284\n",
      "tale: 2264\n",
      "scum: 2255\n",
      "crack: 2237\n",
      "plastic: 2213\n",
      "toilet: 2197\n",
      "tourist: 2165\n",
      "loaded: 2141\n",
      "trick: 2140\n",
      "trap: 2101\n",
      "dust: 2096\n",
      "tight: 2092\n",
      "joint: 2053\n",
      "bait: 2052\n",
      "lefty: 2046\n",
      "snowflake: 2043\n",
      "bird: 2015\n",
      "karen: 1990\n",
      "bent: 1978\n",
      "plot: 1969\n",
      "tory: 1953\n",
      "confident: 1914\n",
      "alien: 1877\n",
      "yellow: 1871\n",
      "meth: 1868\n",
      "salt: 1864\n",
      "pussy: 1845\n",
      "fruit: 1805\n",
      "tribe: 1782\n",
      "rough: 1743\n",
      "globalist: 1738\n",
      "chronic: 1737\n",
      "asshole: 1712\n",
      "twist: 1679\n",
      "sickening: 1673\n",
      "bust: 1673\n",
      "pointless: 1662\n",
      "billy: 1617\n",
      "pandemic: 1576\n",
      "creep: 1556\n",
      "bump: 1540\n",
      "chip: 1537\n",
      "filthy: 1527\n",
      "rotten: 1516\n",
      "drone: 1501\n",
      "beef: 1488\n",
      "shed: 1474\n",
      "dictionary: 1474\n",
      "bare: 1443\n",
      "crow: 1416\n",
      "dutch: 1416\n",
      "shine: 1407\n",
      "gentleman: 1394\n",
      "bang: 1381\n",
      "spew: 1380\n",
      "mill: 1378\n",
      "olds: 1367\n",
      "raid: 1361\n",
      "pseudo: 1344\n",
      "snake: 1342\n",
      "goose: 1336\n",
      "ease: 1327\n",
      "pork: 1318\n",
      "johnny: 1295\n",
      "chump: 1243\n",
      "crush: 1240\n",
      "dope: 1233\n",
      "thick: 1230\n",
      "iron: 1225\n",
      "fairy: 1221\n",
      "pony: 1210\n",
      "beast: 1205\n",
      "blast: 1196\n",
      "bucket: 1183\n",
      "agricultural: 1181\n",
      "shill: 1169\n",
      "cattle: 1165\n",
      "herd: 1163\n",
      "hype: 1153\n",
      "meme: 1147\n",
      "freaking: 1145\n",
      "plug: 1141\n",
      "shower: 1118\n",
      "sack: 1101\n",
      "burger: 1101\n",
      "brigade: 1096\n",
      "farmer: 1089\n",
      "baggage: 1084\n",
      "rainbow: 1084\n",
      "crook: 1081\n",
      "dodge: 1073\n",
      "madam: 1067\n",
      "screwing: 1065\n",
      "drunken: 1043\n",
      "carpet: 1039\n",
      "commie: 1038\n",
      "idiocy: 1037\n",
      "freak: 1036\n",
      "monkey: 1030\n",
      "ghost: 1025\n",
      "stink: 1000\n",
      "vulgar: 989\n",
      "cabal: 986\n",
      "scrap: 967\n",
      "colonialism: 962\n",
      "sandy: 950\n",
      "mortal: 929\n",
      "banana: 920\n",
      "slick: 905\n",
      "yahoo: 900\n",
      "chill: 900\n",
      "casual: 887\n",
      "punk: 881\n",
      "perverted: 871\n",
      "filth: 854\n",
      "bastard: 854\n",
      "drip: 852\n",
      "cope: 851\n",
      "princess: 849\n",
      "melt: 828\n",
      "grind: 806\n",
      "wagon: 803\n",
      "curse: 799\n",
      "shark: 795\n",
      "tiger: 792\n",
      "sucker: 788\n",
      "creature: 784\n",
      "brick: 780\n",
      "buffalo: 778\n",
      "lump: 767\n",
      "ivory: 764\n",
      "vagina: 760\n",
      "baked: 748\n",
      "carnage: 746\n",
      "squat: 739\n",
      "bacon: 736\n",
      "blunt: 736\n",
      "weasel: 733\n",
      "invalid: 732\n",
      "randy: 731\n",
      "explosive: 726\n",
      "manipulative: 723\n",
      "wicked: 717\n",
      "breeding: 715\n",
      "ramp: 714\n",
      "reddit: 706\n",
      "poop: 705\n",
      "shrink: 704\n",
      "stiff: 699\n",
      "colored: 699\n",
      "race card: 699\n",
      "tricky: 697\n",
      "operator: 692\n",
      "psycho: 678\n",
      "copper: 669\n",
      "ghetto: 662\n",
      "stan: 662\n",
      "outlaw: 652\n",
      "cage: 649\n",
      "malignant: 645\n",
      "butcher: 640\n",
      "nigga: 634\n",
      "loot: 633\n",
      "slime: 632\n",
      "blather: 627\n",
      "willy: 622\n",
      "handout: 620\n",
      "tripe: 606\n",
      "whack: 605\n",
      "dame: 603\n",
      "skirt: 603\n",
      "slant: 600\n",
      "puff: 595\n",
      "puppy: 594\n",
      "dummy: 591\n",
      "curry: 589\n",
      "marxism: 587\n",
      "fluff: 577\n",
      "nice guy: 574\n",
      "slash: 573\n",
      "crock: 569\n",
      "savage: 568\n",
      "zombie: 565\n",
      "villain: 565\n",
      "muck: 558\n",
      "curtain: 557\n",
      "slimy: 555\n",
      "homo: 547\n",
      "mormon: 543\n",
      "handicapped: 542\n",
      "cowboy: 542\n",
      "fever: 538\n",
      "intake: 530\n",
      "dragon: 527\n",
      "imbecile: 512\n",
      "stab: 507\n",
      "ethiopian: 507\n",
      "shade: 505\n",
      "stinking: 504\n",
      "big fat: 503\n",
      "tossing: 500\n",
      "demagogue: 495\n",
      "parasite: 494\n",
      "fierce: 480\n",
      "shocker: 467\n",
      "pagan: 459\n",
      "redneck: 454\n",
      "blasted: 448\n",
      "cock: 448\n",
      "brat: 446\n",
      "heel: 444\n",
      "nimby: 440\n",
      "mole: 440\n",
      "sympathizer: 435\n",
      "fart: 423\n",
      "graft: 420\n",
      "dome: 417\n",
      "faggot: 417\n",
      "wool: 416\n",
      "doll: 414\n",
      "lash: 413\n",
      "voodoo: 407\n",
      "horny: 407\n",
      "betty: 405\n",
      "retarded: 400\n",
      "imposition: 391\n",
      "old woman: 391\n",
      "banging: 388\n",
      "bacteria: 384\n",
      "rabble: 383\n",
      "pimp: 377\n",
      "cronyism: 370\n",
      "puke: 364\n",
      "gomer: 362\n",
      "beaver: 361\n",
      "flaming: 353\n",
      "cripple: 353\n",
      "shaft: 349\n",
      "skinner: 348\n",
      "contra: 347\n",
      "carrot: 346\n",
      "senile: 345\n",
      "joker: 344\n",
      "piggy: 342\n",
      "sketchy: 340\n",
      "anal: 334\n",
      "mooch: 333\n",
      "bummer: 331\n",
      "douche: 324\n",
      "muzzle: 322\n",
      "mandarin: 321\n",
      "blackout: 318\n",
      "whitey: 316\n",
      "crippled: 315\n",
      "jackass: 314\n",
      "trot: 311\n",
      "missionary: 311\n",
      "frog: 310\n",
      "flimsy: 304\n",
      "vegetable: 303\n",
      "dread: 302\n",
      "greener: 301\n",
      "septic: 300\n",
      "shrill: 300\n",
      "slump: 299\n",
      "hillbilly: 298\n",
      "mickey: 297\n",
      "parochial: 290\n",
      "handicap: 285\n",
      "swine: 284\n",
      "spawn: 282\n",
      "diaper: 279\n",
      "joey: 279\n",
      "figurehead: 276\n",
      "buggy: 276\n",
      "plunder: 274\n",
      "mundane: 272\n",
      "autistic: 272\n",
      "cracker: 270\n",
      "gouge: 269\n",
      "deadbeat: 267\n",
      "yuck: 266\n",
      "goddamn: 265\n",
      "taint: 265\n",
      "combo: 263\n",
      "prick: 262\n",
      "middle of the road: 258\n",
      "vampire: 257\n",
      "grill: 256\n",
      "feeder: 253\n",
      "sticky: 252\n",
      "gangster: 250\n",
      "skunk: 250\n",
      "facile: 249\n",
      "goon: 244\n",
      "scheming: 243\n",
      "napoleon: 243\n",
      "crusader: 235\n",
      "salty: 233\n",
      "sicko: 230\n",
      "cavalier: 230\n",
      "squash: 227\n",
      "cutter: 225\n",
      "sugary: 223\n",
      "barker: 222\n",
      "soapbox: 221\n",
      "boob: 221\n",
      "nigger: 219\n",
      "blinding: 219\n",
      "greasy: 217\n",
      "hurl: 215\n",
      "benny: 213\n",
      "bong: 207\n",
      "yankee: 205\n",
      "nutter: 204\n",
      "fluffy: 203\n",
      "mack: 201\n",
      "wonk: 201\n",
      "bloat: 199\n",
      "inbred: 199\n",
      "eskimo: 198\n",
      "scrub: 196\n",
      "weirdo: 192\n",
      "coconut: 192\n",
      "hooker: 191\n",
      "goof: 189\n",
      "quack: 187\n",
      "cram: 184\n",
      "howler: 183\n",
      "shrimp: 183\n",
      "warmonger: 183\n",
      "wack: 182\n",
      "brute: 182\n",
      "pike: 179\n",
      "throwback: 179\n",
      "empty suit: 179\n",
      "wimp: 178\n",
      "muckamuck: 178\n",
      "diabolical: 177\n",
      "fat cat: 176\n",
      "clown car: 173\n",
      "cull: 171\n",
      "derelict: 169\n",
      "bender: 167\n",
      "cockroach: 166\n",
      "snatch: 165\n",
      "cancerous: 163\n",
      "mule: 163\n",
      "tranny: 161\n",
      "boiler: 160\n",
      "snot: 159\n",
      "dwarf: 159\n",
      "mayo: 159\n",
      "clam: 159\n",
      "sissy: 159\n",
      "leery: 158\n",
      "zebra: 155\n",
      "fatty: 155\n",
      "crust: 155\n",
      "gorge: 155\n",
      "alimony: 154\n",
      "slop: 154\n",
      "hoodie: 153\n",
      "jock: 151\n",
      "pothead: 150\n",
      "geezer: 149\n",
      "tinker: 148\n",
      "son of a bitch: 148\n",
      "diddly: 147\n",
      "rodent: 147\n",
      "dusty: 147\n",
      "dino: 145\n",
      "ninja: 143\n",
      "cash grab: 142\n",
      "pimping: 139\n",
      "foist: 139\n",
      "ginger: 138\n",
      "ratchet: 138\n",
      "dong: 136\n",
      "snob: 136\n",
      "digger: 136\n",
      "scummy: 134\n",
      "bugger: 133\n",
      "midget: 131\n",
      "babu: 128\n",
      "scoff: 128\n",
      "rumble: 127\n",
      "slit: 126\n",
      "thuggish: 125\n",
      "putz: 125\n",
      "juggernaut: 125\n",
      "barbarian: 125\n",
      "dink: 125\n",
      "peasant: 124\n",
      "sound bite: 122\n",
      "thrashing: 121\n",
      "proletariat: 120\n",
      "bourgeois: 119\n",
      "cooker: 119\n",
      "cabbage: 118\n",
      "legalism: 117\n",
      "darren: 116\n",
      "rooster: 115\n",
      "heavy-handed: 115\n",
      "spank: 114\n",
      "ding dong: 113\n",
      "knob: 110\n",
      "effeminate: 110\n",
      "hump: 109\n",
      "lush: 109\n",
      "twink: 109\n",
      "tramp: 107\n",
      "magician: 106\n",
      "dialect: 103\n",
      "fishing expedition: 102\n",
      "crumb: 100\n",
      "prod: 99\n",
      "femme: 98\n",
      "breeder: 97\n",
      "journeyman: 96\n",
      "mutt: 95\n",
      "roach: 94\n",
      "ecstasy: 91\n",
      "scab: 90\n",
      "paddy: 89\n",
      "spook: 88\n",
      "copycat: 88\n",
      "metaphysics: 88\n",
      "gypsy: 88\n",
      "toaster: 86\n",
      "tang: 86\n",
      "mare: 86\n",
      "moth: 85\n",
      "trucker: 85\n",
      "derp: 85\n",
      "squirt: 84\n",
      "pansy: 84\n",
      "horse manure: 84\n",
      "shanty: 83\n",
      "fanny: 83\n",
      "itchy: 82\n",
      "troglodyte: 80\n",
      "creeper: 79\n",
      "scat: 78\n",
      "goofball: 78\n",
      "dork: 78\n",
      "flunky: 78\n",
      "devolution: 77\n",
      "pseudoscience: 77\n",
      "canuck: 77\n",
      "weenie: 76\n",
      "silo: 75\n",
      "maggot: 74\n",
      "sieve: 74\n",
      "wanker: 74\n",
      "watermelon: 74\n",
      "scratcher: 72\n",
      "gramps: 72\n",
      "slacker: 71\n",
      "man child: 71\n",
      "crate: 71\n",
      "puss: 71\n",
      "humpback: 69\n",
      "kisser: 69\n",
      "snout: 68\n",
      "harem: 67\n",
      "pinhead: 67\n",
      "luddite: 67\n",
      "white knight: 67\n",
      "gull: 66\n",
      "skid row: 66\n",
      "booby: 65\n",
      "squid: 65\n",
      "donk: 65\n",
      "riffraff: 64\n",
      "diva: 63\n",
      "hunky: 63\n",
      "big girl: 63\n",
      "incel: 63\n",
      "mouthy: 63\n",
      "baby boy: 63\n",
      "ambulance chaser: 63\n",
      "stinker: 62\n",
      "statism: 62\n",
      "divvy: 62\n",
      "boner: 60\n",
      "dildo: 60\n",
      "jockey: 60\n",
      "crusher: 59\n",
      "greenie: 59\n",
      "babylonian: 58\n",
      "paki: 58\n",
      "coon: 57\n",
      "drab: 56\n",
      "hippo: 56\n",
      "vulcan: 55\n",
      "jezebel: 55\n",
      "pasty: 55\n",
      "bruising: 55\n",
      "wildcat: 55\n",
      "high and dry: 54\n",
      "stank: 54\n",
      "chummy: 54\n",
      "smut: 54\n",
      "black diamond: 54\n",
      "percy: 54\n",
      "junker: 53\n",
      "skank: 53\n",
      "jerk off: 53\n",
      "dilettante: 53\n",
      "simp: 53\n",
      "hummer: 53\n",
      "cardboard box: 52\n",
      "snit: 52\n",
      "nelly: 51\n",
      "scarecrow: 51\n",
      "doper: 50\n",
      "tattle: 50\n",
      "white bread: 50\n",
      "harpy: 50\n",
      "shoehorn: 49\n",
      "gook: 48\n",
      "retardation: 47\n",
      "spunk: 47\n",
      "the sticks: 47\n",
      "beastly: 47\n",
      "lech: 46\n",
      "blubber: 45\n",
      "careerist: 45\n",
      "fruity: 44\n",
      "hoof: 44\n",
      "queerness: 44\n",
      "pecker: 44\n",
      "butchery: 43\n",
      "tart: 42\n",
      "spud: 41\n",
      "dipstick: 41\n",
      "moocher: 41\n",
      "viper: 41\n",
      "nance: 40\n",
      "rustic: 40\n",
      "oiler: 39\n",
      "wench: 39\n",
      "hooligan: 38\n",
      "misbegotten: 38\n",
      "macaroni: 37\n",
      "gnome: 37\n",
      "mongol: 37\n",
      "honky: 36\n",
      "shitter: 36\n",
      "porcine: 36\n",
      "money machine: 36\n",
      "hothead: 35\n",
      "prissy: 35\n",
      "booger: 35\n",
      "hoon: 35\n",
      "shag: 34\n",
      "schizoid: 34\n",
      "arian: 34\n",
      "pouch: 34\n",
      "chug: 33\n",
      "locust: 33\n",
      "moralism: 33\n",
      "sucky: 33\n",
      "popolo: 33\n",
      "gronk: 32\n",
      "mongrel: 32\n",
      "wet blanket: 32\n",
      "tosh: 32\n",
      "batty: 31\n",
      "newfangled: 31\n",
      "old money: 31\n",
      "twinkie: 31\n",
      "harlot: 31\n",
      "drifter: 30\n",
      "bussy: 30\n",
      "dunny: 30\n",
      "xian: 30\n",
      "scuffle: 30\n",
      "eggplant: 29\n",
      "buckwheat: 29\n",
      "scurvy: 29\n",
      "titty: 29\n",
      "saccharine: 29\n",
      "bigmouth: 28\n",
      "polemical: 28\n",
      "ghoul: 28\n",
      "standard issue: 28\n",
      "strumpet: 27\n",
      "sodomite: 27\n",
      "wrangler: 27\n",
      "cyclops: 27\n",
      "melon: 27\n",
      "full retard: 25\n",
      "monopoly money: 25\n",
      "runt: 25\n",
      "galilean: 25\n",
      "moralist: 24\n",
      "potentate: 24\n",
      "gash: 23\n",
      "mouldy: 23\n",
      "vamp: 23\n",
      "nonce: 23\n",
      "prat: 23\n",
      "carrion: 23\n",
      "softy: 23\n",
      "hackery: 23\n",
      "smooch: 22\n",
      "piker: 22\n",
      "churchy: 22\n",
      "hovel: 22\n",
      "glory hole: 22\n",
      "rook: 22\n",
      "kafir: 22\n",
      "miser: 21\n",
      "khaki: 21\n",
      "bubblegum: 21\n",
      "gang bang: 21\n",
      "showy: 21\n",
      "whelp: 20\n",
      "shemale: 20\n",
      "rando: 20\n",
      "jizz: 20\n",
      "egghead: 20\n",
      "fash: 20\n",
      "jacky: 20\n",
      "sewer rat: 20\n",
      "chinaman: 19\n",
      "woo woo: 19\n",
      "porker: 18\n",
      "squish: 18\n",
      "arty: 18\n",
      "pleb: 18\n",
      "spinster: 18\n",
      "clown world: 18\n",
      "trixie: 17\n",
      "tyke: 17\n",
      "gaylord: 17\n",
      "spic: 17\n",
      "mozzarella: 17\n",
      "grossness: 17\n",
      "nouveau riche: 17\n",
      "black gang: 17\n",
      "muzzy: 17\n",
      "pygmy: 17\n",
      "twee: 16\n",
      "hireling: 16\n",
      "piddle: 16\n",
      "moke: 16\n",
      "bossman: 16\n",
      "toots: 16\n",
      "gett: 16\n",
      "biddy: 16\n",
      "puta: 16\n",
      "puffer: 15\n",
      "waspy: 15\n",
      "buzzard: 15\n",
      "bougie: 15\n",
      "space opera: 15\n",
      "careerism: 15\n",
      "daddy's girl: 14\n",
      "bubby: 14\n",
      "poon: 14\n",
      "coker: 14\n",
      "plebeian: 13\n",
      "pissant: 13\n",
      "hoser: 13\n",
      "professional student: 13\n",
      "hocus-pocus: 13\n",
      "mystery meat: 12\n",
      "scuttlebutt: 12\n",
      "sook: 12\n",
      "gilt: 12\n",
      "bruiser: 12\n",
      "chud: 12\n",
      "jackal: 12\n",
      "sulky: 11\n",
      "wifebeater: 11\n",
      "wino: 11\n",
      "swarthy: 11\n",
      "cocking: 11\n",
      "pomegranate: 11\n",
      "hinky: 11\n",
      "beached whale: 11\n",
      "bogan: 11\n",
      "headcase: 10\n",
      "hooter: 10\n",
      "sprig: 10\n",
      "schoolmarm: 10\n",
      "drop kick: 10\n",
      "politique: 10\n",
      "dingus: 10\n",
      "fenian: 10\n",
      "blither: 10\n",
      "townie: 9\n",
      "po-po: 9\n",
      "shiner: 9\n",
      "dingleberry: 9\n",
      "stiffy: 9\n",
      "the other place: 9\n",
      "magpie: 9\n",
      "lamebrain: 9\n",
      "dutchman: 9\n",
      "peckerwood: 9\n",
      "low-down: 8\n",
      "prawn: 8\n",
      "fakir: 8\n",
      "tosser: 8\n",
      "redskin: 8\n",
      "wonderbread: 8\n",
      "coochie: 8\n",
      "dauber: 8\n",
      "wigger: 8\n",
      "bung: 8\n",
      "succ: 8\n",
      "party school: 7\n",
      "pisser: 7\n",
      "one joke: 7\n",
      "knocker: 7\n",
      "bloodsucker: 7\n",
      "pseud: 7\n",
      "baby-killer: 7\n",
      "oinker: 7\n",
      "buggery: 7\n",
      "boink: 7\n",
      "cooter: 7\n",
      "old maid: 7\n",
      "limey: 6\n",
      "papoose: 6\n",
      "minx: 6\n",
      "short stuff: 6\n",
      "wowser: 6\n",
      "hee-haw: 6\n",
      "pocket pool: 6\n",
      "woker: 5\n",
      "scalawag: 5\n",
      "snoot: 5\n",
      "middlebrow: 5\n",
      "frit: 5\n",
      "peeler: 5\n",
      "fishwife: 5\n",
      "sorehead: 5\n",
      "quim: 5\n",
      "scally: 5\n",
      "romanist: 5\n",
      "weak sister: 5\n",
      "herm: 5\n",
      "jumped-up: 5\n",
      "wifie: 4\n",
      "subnormal: 4\n",
      "tonk: 4\n",
      "baby face: 4\n",
      "cunty: 4\n",
      "bolshie: 4\n",
      "grotty: 4\n",
      "beaner: 4\n",
      "cootie: 4\n",
      "bleeder: 4\n",
      "khazar: 4\n",
      "bell ringer: 4\n",
      "half-breed: 4\n",
      "slattern: 4\n",
      "tryhard: 4\n",
      "mugwump: 4\n",
      "scalie: 4\n",
      "aborter: 3\n",
      "asur: 3\n",
      "bumf: 3\n",
      "blackwashing: 3\n",
      "blue-eyed boy: 3\n",
      "westie: 3\n",
      "rain man: 3\n",
      "chiseler: 3\n",
      "streetwalker: 3\n",
      "assy: 3\n",
      "one-man band: 3\n",
      "dickies: 3\n",
      "wobbler: 3\n",
      "pelf: 3\n",
      "ragamuffin: 3\n",
      "chrome dome: 3\n",
      "pogue: 3\n",
      "moll: 3\n",
      "dip stick: 3\n",
      "foofoo: 2\n",
      "peeper: 2\n",
      "cathouse: 2\n",
      "jelly roll: 2\n",
      "jammy: 2\n",
      "clanker: 2\n",
      "indian time: 2\n",
      "smearer: 2\n",
      "banderite: 2\n",
      "buttinsky: 2\n",
      "nicker: 2\n",
      "mong: 2\n",
      "brach: 2\n",
      "kinglet: 2\n",
      "fathead: 2\n",
      "black gangster: 2\n",
      "nipper: 2\n",
      "pea patch: 2\n",
      "scummer: 2\n",
      "cunny: 2\n",
      "flea bag: 2\n",
      "chinky: 2\n",
      "termagant: 2\n",
      "derm: 2\n",
      "minute man: 2\n",
      "knacker: 2\n",
      "scullion: 2\n",
      "mucker: 2\n",
      "butterball: 2\n",
      "snakehead: 2\n",
      "moggy: 2\n",
      "white monkey: 1\n",
      "squawker: 1\n",
      "dogface: 1\n",
      "obeast: 1\n",
      "golliwog: 1\n",
      "chickenhead: 1\n",
      "epicene: 1\n",
      "boffing: 1\n",
      "spooge: 1\n",
      "quiff: 1\n",
      "bashaw: 1\n",
      "keeno: 1\n",
      "wrong 'un: 1\n",
      "rockhead: 1\n",
      "time vampire: 1\n",
      "wanger: 1\n",
      "cuke: 1\n",
      "munter: 1\n",
      "scallywag: 1\n",
      "dog robber: 1\n",
      "senga: 1\n",
      "dopper: 1\n",
      "awokening: 1\n",
      "carbage: 1\n",
      "flunkey: 1\n",
      "vonce: 1\n",
      "blueshirt: 1\n",
      "stinkpot: 1\n",
      "pisher: 1\n",
      "stumblebum: 1\n",
      "corn-cracker: 0\n",
      "schafskopf: 0\n",
      "fatface: 0\n",
      "musk cat: 0\n",
      "hindoo: 0\n",
      "musclebound: 0\n",
      "hoojah: 0\n",
      "bolitics: 0\n",
      "no-life: 0\n",
      "paradoxist: 0\n",
      "guppie: 0\n",
      "bibler: 0\n",
      "bludger: 0\n",
      "red nigger: 0\n",
      "creeker: 0\n",
      "ocker: 0\n",
      "ham-and-egger: 0\n",
      "cummer: 0\n",
      "mombie: 0\n",
      "dappa: 0\n",
      "gobby: 0\n",
      "welshy: 0\n",
      "social imperialism: 0\n",
      "niggerhead: 0\n",
      "gutter dog: 0\n",
      "zip coon: 0\n",
      "noodler: 0\n",
      "ploppy: 0\n",
      "barse: 0\n",
      "hockle: 0\n",
      "clart: 0\n",
      "chinese home run: 0\n",
      "greenshirt: 0\n",
      "besom: 0\n",
      "shaveling: 0\n",
      "gallicanism: 0\n",
      "turnspit: 0\n",
      "cawk: 0\n",
      "paskudnyak: 0\n",
      "whoremaster: 0\n",
      "effable: 0\n",
      "mameluke: 0\n",
      "foamer: 0\n",
      "bluestocking: 0\n",
      "bamp: 0\n",
      "box checker: 0\n",
      "broadbrim: 0\n",
      "chugger: 0\n",
      "fleshling: 0\n",
      "holidayism: 0\n",
      "waste of breath: 0\n",
      "bluey: 0\n",
      "lime-juicer: 0\n",
      "wokery: 0\n",
      "yiddo: 0\n",
      "uniform queen: 0\n",
      "scrag: 0\n",
      "dinge: 0\n",
      "fagdom: 0\n",
      "beaver eater: 0\n",
      "sprog: 0\n",
      "paddyism: 0\n",
      "flatfoot: 0\n",
      "lardy: 0\n",
      "gippo: 0\n",
      "crossback: 0\n",
      "walloper: 0\n",
      "bantling: 0\n",
      "frigger: 0\n",
      "brown eye: 0\n",
      "cheese bus: 0\n",
      "kugel: 0\n",
      "four by two: 0\n",
      "pommy: 0\n",
      "magus: 0\n",
      "buckra: 0\n",
      "chanate: 0\n",
      "radge: 0\n",
      "bogger: 0\n",
      "bogtrotter: 0\n",
      "nipcheese: 0\n",
      "clitty: 0\n",
      "rice king: 0\n",
      "chevisance: 0\n",
      "queenlet: 0\n",
      "womanish: 0\n",
      "yam yam: 0\n",
      "new chum: 0\n",
      "junkhead: 0\n",
      "toe rag: 0\n",
      "franquist: 0\n",
      "nobber: 0\n",
      "unteacher: 0\n",
      "hindian: 0\n",
      "milk drinker: 0\n",
      "raggie: 0\n",
      "dobber: 0\n",
      "speccy: 0\n",
      "tarrier: 0\n",
      "booner: 0\n",
      "junkball: 0\n",
      "repper: 0\n",
      "cheeser: 0\n",
      "hoopy: 0\n",
      "bristler: 0\n",
      "preciosity: 0\n",
      "roider: 0\n",
      "free-willer: 0\n",
      "goback: 0\n",
      "cornfed: 0\n",
      "marmoset: 0\n",
      "mud shark: 0\n",
      "whipstitch: 0\n",
      "dudelet: 0\n",
      "pussydom: 0\n",
      "hagiographer: 0\n",
      "bourgie: 0\n",
      "golden ear: 0\n",
      "banana boat: 0\n",
      "body snatcher: 0\n",
      "cruff: 0\n",
      "grass-eater: 0\n",
      "mackem: 0\n",
      "chirper: 0\n",
      "hatchet wound: 0\n",
      "fancy man: 0\n",
      "sheeny: 0\n",
      "gegger: 0\n",
      "fleshpot: 0\n",
      "rhodie: 0\n",
      "tutti-frutti: 0\n",
      "funny money: 0\n",
      "paddywhack: 0\n",
      "squit: 0\n",
      "scrote: 0\n",
      "painted jezebel: 0\n",
      "whoreson: 0\n",
      "charva: 0\n",
      "jehovist: 0\n",
      "nitchie: 0\n",
      "bummery: 0\n",
      "pickaninny christmas: 0\n",
      "pickaninny: 0\n",
      "skag: 0\n",
      "pot boiler: 0\n"
     ]
    }
   ],
   "source": [
    "for key, value in sorted(counts.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c99da4",
   "metadata": {},
   "source": [
    "### FILTERING ON LND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2e0274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7c9953ec144d719d793d87765cbbb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_dir=\"mydataset/LND\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a5efa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\lexicon.json', 'r') as f:\n",
    "    lexicon = json.load(f)\n",
    "    lexicon = set(w['word'].lower() for w in lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d860a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = dataset['word']\n",
    "counts = Counter(words)\n",
    "\n",
    "zero_count_words = [word for word in lexicon if counts.get(word, 0) == 0]\n",
    "\n",
    "result = {word: counts.get(word, 0) for word in lexicon}\n",
    "sorted_result = dict(sorted(result.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Stampa in ordine decrescente\n",
    "for word, count in sorted_result.items():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb6e1fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wokery', 'greenshirt', 'jehovist', 'bourgie', 'musk cat', 'queenlet', 'guppie', 'walloper', 'paddyism', 'gippo', 'bogger', 'frigger', 'hoopy', 'ham-and-egger', 'lime-juicer', 'fleshpot', 'pommy', 'turnspit', 'holidayism', 'grass-eater', 'gobby', 'rhodie', 'red nigger', 'magus', 'paskudnyak', 'milk drinker', 'shaveling', 'fagdom', 'yiddo', 'kugel', 'rice king', 'hindoo', 'gallicanism', 'social imperialism', 'whipstitch', 'hindian', 'mud shark', 'pickaninny', 'dobber', 'yam yam', 'new chum', 'noodler', 'golden ear', 'paradoxist', 'besom', 'broadbrim', 'roider', 'crossback', 'tutti-frutti', 'booner', 'chugger', 'hagiographer', 'niggerhead', 'fleshling', 'uniform queen', 'mameluke', 'scrote', 'repper', 'chevisance', 'whoremaster', 'pot boiler', 'cornfed', 'sprog', 'raggie', 'box checker', 'marmoset', 'corn-cracker', 'fatface', 'clitty', 'unteacher', 'painted jezebel', 'pickaninny christmas', 'lardy', 'zip coon', 'radge', 'bibler', 'nitchie', 'bluestocking', 'chanate', 'dudelet', 'bristler', 'charva', 'sheeny', 'bolitics', 'chirper', 'womanish', 'hoojah', 'tarrier', 'whoreson', 'foamer', 'bogtrotter', 'no-life', 'speccy', 'franquist', 'ploppy', 'beaver eater', 'creeker', 'flatfoot', 'mombie', 'cheese bus', 'bummery', 'bantling', 'schafskopf', 'cruff', 'paddywhack', 'pussydom', 'body snatcher', 'clart', 'junkball', 'nipcheese', 'free-willer', 'gegger', 'dappa', 'cheeser', 'bludger', 'squit', 'nobber', 'bamp', 'preciosity', 'toe rag', 'welshy', 'chinese home run', 'buckra', 'goback', 'scrag', 'bluey', 'mackem', 'musclebound', 'gutter dog']\n"
     ]
    }
   ],
   "source": [
    "print(zero_count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be474d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\lexicon.json', 'r') as f:\n",
    "    full_lexicon = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28075b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = [entry for entry in full_lexicon if entry['word'].lower() not in zero_count_words]\n",
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\LND_lexicon.json', 'w') as f:\n",
    "    json.dump(full_lexicon, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8712101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1daddc4840b94e8ca480ccd880708372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### DATASET LOADED  ######################\n",
      "################### LEXICON LOADED  ######################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff6f2fe18ed4227ba3bd5bf8957e7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=1):   0%|          | 0/1562 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a variable of type <class 'numpy.ndarray'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\utils\\py_utils.py\", line 586, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3683, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n  File \"c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3633, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n  File \"c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3557, in apply_function\n    return prepare_outputs(pa_inputs, inputs, processed_inputs)\n  File \"c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3521, in prepare_outputs\n    validate_function_output(processed_inputs)\n  File \"c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 3461, in validate_function_output\n    raise TypeError(\nTypeError: Provided `function` which is applied to all elements of table returns a variable of type <class 'numpy.ndarray'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m lexicon:\n\u001b[0;32m     34\u001b[0m     examples \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m example, w\u001b[38;5;241m=\u001b[39mword: example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m w,\n\u001b[0;32m     36\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     37\u001b[0m     )\n\u001b[1;32m---> 38\u001b[0m     embeddings[word] \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fatto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(dataset, model)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(dataset, model):\n\u001b[1;32m---> 13\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:3318\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[0;32m   3315\u001b[0m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;241m=\u001b[39m prev_env\n\u001b[0;32m   3316\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3319\u001b[0m     pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39munprocessed_kwargs_per_job\n\u001b[0;32m   3320\u001b[0m ):\n\u001b[0;32m   3321\u001b[0m     check_if_shard_done(rank, done, content)\n\u001b[0;32m   3323\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\utils\\py_utils.py:626\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 626\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\Dario\\miniconda3\\lib\\site-packages\\datasets\\utils\\py_utils.py:626\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 626\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mc:\\Users\\Dario\\miniconda3\\lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a variable of type <class 'numpy.ndarray'>. Make sure provided `function` returns a variable of type `dict` (or a pyarrow table) to update the dataset or `None` if you are only interested in side effects."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "\n",
    "def get_lexicon() -> set:\n",
    "    with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\LND_lexicon.json', 'r') as f:\n",
    "        lexicon = json.load(f)\n",
    "\n",
    "    return set(w['word'].lower() for w in lexicon)\n",
    "\n",
    "def get_embedding(dataset, model):\n",
    "    embeddings = dataset.map(\n",
    "        lambda batch, m=model: m.encode(batch['text']),\n",
    "        num_proc = 1,\n",
    "        batched = True\n",
    "    )\n",
    "    return embeddings.mean(dim=1) # Pooling\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset = load_dataset('parquet', data_dir='mydatasets/LND', split='train')\n",
    "    print(\"################### DATASET LOADED  ######################\")\n",
    "    \n",
    "    lexicon = get_lexicon()\n",
    "    print(\"################### LEXICON LOADED  ######################\")\n",
    "\n",
    "    model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=0)\n",
    "\n",
    "    embeddings = {}\n",
    "\n",
    "    for word in lexicon:\n",
    "        examples = dataset.filter(\n",
    "            lambda example, w=word: example['word'].lower() == w,\n",
    "            num_proc=1\n",
    "        )\n",
    "        embeddings[word] = get_embedding(examples, model)\n",
    "        print(f\"{word} fatto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3e2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
