{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9dc7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "\n",
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\lexicon.json', 'r') as f:\n",
    "    lexicon = json.load(f)\n",
    "    lexicon = set(w['word'].lower() for w in lexicon)\n",
    "\n",
    "counts = {word: 0 for word in lexicon}\n",
    "\n",
    "pattern = re.compile(\n",
    "    r'\\b(' + '|'.join(re.escape(word) for word in lexicon) + r')\\b',\n",
    "    re.IGNORECASE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbb97c",
   "metadata": {},
   "source": [
    "CivilComments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e51a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CivilComments/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50575342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999516/1999516 [46:46<00:00, 712.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['comment_text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254c64d",
   "metadata": {},
   "source": [
    "MeToo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f4aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "df = pd.read_csv(\"MeToo/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74fd197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 807174/807174 [07:56<00:00, 1692.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f91e8",
   "metadata": {},
   "source": [
    "HS Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bc6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hst_classes = [\"Hate\", \"Normal\", \"Offensive\"]\n",
    "hst_dfs = []\n",
    "\n",
    "for cls in hst_classes:\n",
    "    for i in range(1, 5):  # files 1 to 4\n",
    "        file_name = f\"HS Tweets/{cls}_Speeches_{i}.csv\"\n",
    "        tmp = pd.read_csv(file_name)\n",
    "        tmp[\"class\"] = cls\n",
    "        hst_dfs.append(tmp)\n",
    "\n",
    "# Combine all files into a single DataFrame\n",
    "df = pd.concat(hst_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b018e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3061/3061 [00:03<00:00, 998.79it/s] \n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['full_text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a05c1",
   "metadata": {},
   "source": [
    "MultiTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9039ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MultiTarget/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2184d3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11243/11243 [00:12<00:00, 932.81it/s] \n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['Text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for word in matches:\n",
    "        counts[word.lower()] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e69c5",
   "metadata": {},
   "source": [
    "Toraman22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3fcc784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dario\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: DtypeWarning: Columns (0,4,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Toraman22/v2.tsv\", sep=\"\\t\")\n",
    "\n",
    "# English Only\n",
    "df = df[df[\"language\"] == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dfb1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102066/102066 [00:58<00:00, 1738.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for text in tqdm.tqdm(df['text']):\n",
    "    try:\n",
    "        matches = pattern.findall(text)\n",
    "\n",
    "        for word in matches:\n",
    "            counts[word.lower()] +=1\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04875e08",
   "metadata": {},
   "source": [
    "GHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.read_csv('GHC/ghc_train.tsv', sep='\\t')\n",
    "df2 = pd.read_csv('GHC/ghc_test.tsv', sep='\\t')\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c99da4",
   "metadata": {},
   "source": [
    "### FILTERING ON LND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_dir=\"C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Datasets\\\\mydatasets\\\\LND\", split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec44e8",
   "metadata": {},
   "source": [
    "Parole che appaiono meno di due volte in LAS apparentemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ce9e1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dizionario = {'little': 57878, 'political': 56316, 'pretty': 31337, 'rape': 24866, 'illegal': 23868, 'moment': 21474, 'easy': 18817, 'funny': 18005, 'piece': 16137, 'waste': 12465, 'fast': 9481, 'negative': 8873, 'tired': 8541, 'cheap': 8245, 'estate': 8087, 'daughter': 7722, 'idiot': 7696, 'hide': 7573, 'african': 7260, 'silly': 7243, 'dumb': 7239, \"people's\": 6746, 'attorney': 6569, 'wide': 6444, 'fool': 6369, 'nazi': 6289, 'fish': 6136, 'busy': 6136, 'trash': 6029, 'slow': 6018, 'drunk': 5617, 'rock': 5551, 'danger': 5421, 'weapon': 5215, 'jewish': 5201, 'suit': 5153, 'useless': 5054, 'ugly': 5024, 'hole': 4901, 'troll': 4817, 'cant': 4778, 'tool': 4635, 'loser': 4401, 'bike': 4200, 'hang': 3967, 'charter': 3950, 'those people': 3750, 'witch': 3245, 'reject': 3220, 'queen': 3206, 'screw': 2946, 'turkey': 2820, 'jerk': 2805, 'bitch': 2736, 'frank': 2686, 'dick': 2565, 'moron': 2561, 'chicken': 2540, 'user': 2526, 'petty': 2420, 'cult': 2399, 'you people': 2258, 'liability': 2226, 'precious': 2175, 'buck': 2126, 'wood': 2091, 'toilet': 2024, 'scum': 2023, 'communism': 2012, 'snowflake': 1923, 'tourist': 1917, 'bent': 1910, 'joint': 1857, 'moose': 1846, 'confident': 1773, 'plastic': 1717, 'blah': 1652, 'yellow': 1648, 'tory': 1619, 'meth': 1611, 'pointless': 1592, 'globalist': 1568, 'pussy': 1567, 'alien': 1562, 'fruit': 1560, 'bust': 1520, 'creep': 1490, 'tribe': 1428, 'dictionary': 1376, 'shed': 1360, 'chip': 1356, 'shine': 1340, 'gentleman': 1324, 'crow': 1291, 'ease': 1265, 'pseudo': 1264, 'olds': 1211, 'mill': 1198, 'pork': 1167, 'drone': 1163, 'asshole': 1152, 'pony': 1140, 'johnny': 1140, 'karen': 1131, 'fairy': 1128, 'chump': 1123, 'bucket': 1120, 'iron': 1117, 'shill': 1115, 'queer': 1089, 'dutch': 1088, 'beast': 1066, 'agricultural': 1052, 'madam': 1023, 'meme': 1005, 'idiocy': 998, 'drunken': 997, 'baggage': 997, 'brigade': 986, 'shower': 983, 'herd': 980, 'burger': 966, 'cattle': 965, 'commie': 956, 'vulgar': 941, 'cabal': 940, 'freak': 922, 'scrap': 909, 'monkey': 898, 'sandy': 854, 'banana': 851, 'casual': 825, 'perverted': 823, 'filth': 805, 'melt': 771, 'colonialism': 748, 'rainbow': 743, 'sucker': 740, 'creature': 736, 'wagon': 724, 'baked': 707, 'vagina': 707, 'weasel': 698, 'curse': 698, 'shark': 698, 'princess': 696, 'brick': 695, 'invalid': 680, 'manipulative': 678, 'shrink': 659, 'race card': 654, 'buffalo': 650, 'colored': 645, 'bacon': 644, 'yahoo': 635, 'bastard': 624, 'ramp': 611, 'psycho': 609, 'pandemic': 604, 'cage': 600, 'malignant': 598, 'tripe': 596, 'blather': 595, 'handout': 590, 'slime': 581, 'slant': 573, 'puff': 564, 'crock': 546, 'curry': 546, 'nice guy': 546, 'curtain': 534, 'ghetto': 532, 'skirt': 531, 'muck': 530, 'marxism': 527, 'puppy': 515, 'dame': 504, 'villain': 503, 'handicapped': 498, 'zombie': 495, 'savage': 492, 'imbecile': 486, 'stan': 469, 'intake': 464, 'big fat': 459, 'fever': 456, 'dragon': 431, 'parasite': 430, 'redneck': 428, 'demagogue': 425, 'drip': 415, 'mormon': 413, 'mole': 405, 'nimby': 404, 'homo': 401, 'sympathizer': 400, 'fart': 396, 'wool': 390, 'pagan': 375, 'retarded': 369, 'voodoo': 361, 'betty': 358, 'cronyism': 354, 'rabble': 352, 'old woman': 348, 'gomer': 343, 'puke': 342, 'reddit': 340, 'nigga': 336, 'bummer': 329, 'piggy': 327, 'cripple': 327, 'beaver': 318, 'contra': 316, 'trot': 308, 'carrot': 308, 'senile': 306, 'bacteria': 306, 'crippled': 304, 'muzzle': 295, 'greener': 291, 'vegetable': 288, 'shrill': 284, 'hillbilly': 282, 'whitey': 281, 'skinner': 277, 'mandarin': 273, 'spawn': 272, 'missionary': 268, 'parochial': 267, 'cock': 265, 'mundane': 265, 'handicap': 264, 'frog': 259, 'cracker': 258, 'yuck': 256, 'deadbeat': 251, 'combo': 250, 'diaper': 248, 'middle of the road': 248, 'facile': 245, 'joey': 242, 'swine': 237, 'feeder': 231, 'goon': 230, 'crusader': 228, 'autistic': 226, 'scheming': 223, 'prick': 221, 'sicko': 221, 'squash': 219, 'soapbox': 218, 'skunk': 216, 'cutter': 212, 'boob': 210, 'napoleon': 206, 'goddamn': 198, 'sugary': 197, 'bong': 193, 'fluffy': 192, 'bloat': 190, 'nutter': 186, 'scrub': 186, 'faggot': 186, 'yankee': 185, 'vampire': 184, 'inbred': 179, 'empty suit': 175, 'warmonger': 175, 'wonk': 175, 'benny': 174, 'wimp': 172, 'fat cat': 171, 'hooker': 170, 'weirdo': 169, 'clown car': 169, 'ethiopian': 166, 'derelict': 162, 'muckamuck': 162, 'pike': 161, 'quack': 160, 'coconut': 158, 'eskimo': 155, 'dwarf': 153, 'throwback': 151, 'bender': 150, 'boiler': 149, 'nigger': 149, 'clam': 149, 'slop': 144, 'cockroach': 142, 'fatty': 142, 'mayo': 141, 'geezer': 140, 'tinker': 139, 'cash grab': 137, 'alimony': 136, 'hoodie': 133, 'jock': 132, 'tranny': 132, 'rodent': 132, 'snob': 131, 'digger': 130, 'sissy': 129, 'pothead': 128, 'bugger': 124, 'ratchet': 124, 'dino': 124, 'putz': 122, 'sound bite': 120, 'peasant': 120, 'juggernaut': 119, 'thuggish': 119, 'midget': 118, 'ninja': 115, 'heavy-handed': 112, 'proletariat': 110, 'ginger': 110, 'legalism': 110, 'babu': 110, 'darren': 108, 'ding dong': 107, 'barbarian': 106, 'dink': 105, 'cooker': 105, 'bourgeois': 103, 'cabbage': 103, 'effeminate': 103, 'fishing expedition': 100, 'magician': 98, 'lush': 97, 'prod': 95, 'son of a bitch': 93, 'dialect': 92, 'tramp': 92, 'mutt': 91, 'zebra': 89, 'journeyman': 87, 'copycat': 86, 'breeder': 85, 'spook': 82, 'horse manure': 82, 'toaster': 82, 'scab': 81, 'troglodyte': 80, 'itchy': 78, 'goofball': 77, 'shanty': 76, 'flunky': 76, 'trucker': 76, 'canuck': 76, 'dork': 75, 'creeper': 75, 'pansy': 74, 'devolution': 74, 'mare': 74, 'wanker': 73, 'weenie': 73, 'metaphysics': 73, 'sieve': 71, 'pseudoscience': 71, 'man child': 70, 'scratcher': 69, 'silo': 69, 'snout': 67, 'gypsy': 67, 'luddite': 66, 'white knight': 65, 'maggot': 63, 'big girl': 63, 'crate': 63, 'ambulance chaser': 63, 'gramps': 62, 'femme': 62, 'humpback': 60, 'riffraff': 60, 'mouthy': 60, 'greenie': 59, 'squid': 56, 'dildo': 55, 'donk': 55, 'statism': 55, 'high and dry': 54, 'dilettante': 53, 'divvy': 53, 'baby boy': 53, 'watermelon': 53, 'incel': 53, 'skank': 52, 'pasty': 52, 'hippo': 52, 'snit': 51, 'cardboard box': 51, 'diva': 51, 'white bread': 50, 'coon': 50, 'babylonian': 50, 'nelly': 50, 'tattle': 49, 'harpy': 49, 'vulcan': 49, 'shoehorn': 48, 'doper': 48, 'stank': 47, 'gook': 47, 'retardation': 47, 'wildcat': 47, 'junker': 47, 'jezebel': 46, 'blubber': 44, 'paki': 44, 'skid row': 44, 'hoof': 43, 'the sticks': 43, 'black diamond': 42, 'moocher': 41, 'scarecrow': 41, 'pecker': 40, 'percy': 40, 'rustic': 40, 'careerist': 40, 'tart': 39, 'dipstick': 39, 'misbegotten': 38, 'wench': 37, 'money machine': 35, 'prissy': 35, 'porcine': 34, 'hoon': 34, 'mongol': 33, 'booger': 33, 'gnome': 33, 'honky': 33, 'oiler': 32, 'wet blanket': 32, 'twink': 32, 'pouch': 32, 'schizoid': 32, 'moralism': 32, 'batty': 31, 'tosh': 31, 'hothead': 31, 'newfangled': 31, 'mongrel': 30, 'old money': 30, 'macaroni': 30, 'dunny': 29, 'simp': 29, 'shitter': 29, 'xian': 29, 'bigmouth': 28, 'harlot': 28, 'polemical': 28, 'chug': 28, 'gronk': 28, 'arian': 28, 'standard issue': 28, 'drifter': 27, 'ghoul': 27, 'scurvy': 27, 'twinkie': 26, 'buckwheat': 26, 'strumpet': 25, 'fruity': 25, 'monopoly money': 25, 'locust': 25, 'saccharine': 25, 'wrangler': 24, 'potentate': 24, 'moralist': 24, 'galilean': 23, 'funny money': 23, 'hackery': 23, 'eggplant': 23, 'carrion': 23, 'hovel': 21, 'miser': 21, 'cyclops': 21, 'smooch': 21, 'khaki': 21, 'showy': 21, 'sodomite': 21, 'egghead': 20, 'softy': 20, 'sewer rat': 20, 'mouldy': 20, 'whelp': 20, 'jacky': 20, 'full retard': 19, 'rando': 19, 'melon': 19, 'gash': 19, 'nonce': 19, 'kafir': 19, 'churchy': 18, 'squish': 18, 'chinaman': 18, 'porker': 18, 'arty': 18, 'bubblegum': 18, 'gaylord': 17, 'tyke': 17, 'nouveau riche': 17, 'pygmy': 16, 'moke': 16, 'toots': 16, 'black gang': 16, 'buzzard': 15, 'bossman': 15, 'spinster': 15, 'hireling': 15, 'puffer': 15, 'clown world': 15, 'woo woo': 14, 'careerism': 14, 'muzzy': 14, \"daddy's girl\": 14, 'queerness': 14, 'spic': 14, 'fash': 14, 'pissant': 13, 'plebeian': 13, 'coker': 13, 'hoser': 13, 'twee': 12, 'mystery meat': 12, 'bruiser': 12, 'hocus-pocus': 12, 'jackal': 12, 'biddy': 12, 'beached whale': 11, 'pleb': 11, 'swarthy': 11, 'sook': 11, 'sulky': 11, 'wino': 11, 'wifebeater': 11, 'hinky': 10, 'gett': 10, 'drop kick': 10, 'politique': 10, 'professional student': 10, 'headcase': 10, 'waste of breath': 9, 'the other place': 9, 'dutchman': 9, 'space opera': 9, 'dingleberry': 9, 'lamebrain': 9, 'po-po': 9, 'townie': 9, 'blither': 9, 'peckerwood': 9, 'bougie': 9, 'bussy': 9, 'wonderbread': 8, 'tosser': 8, 'fakir': 8, 'trixie': 8, 'shemale': 8, 'dauber': 8, 'bloodsucker': 7, 'wigger': 7, 'redskin': 7, 'oinker': 7, 'party school': 7, 'baby-killer': 7, 'dingus': 7, 'pseud': 7, 'old maid': 7, 'sprig': 7, 'fenian': 7, 'bogan': 7, 'one joke': 7, 'wowser': 6, 'banana boat': 6, 'short stuff': 6, 'limey': 6, 'papoose': 6, 'prawn': 5, 'succ': 5, 'herm': 5, 'scalawag': 5, 'weak sister': 5, 'snoot': 5, 'jumped-up': 5, 'minx': 5, 'fishwife': 5, 'chud': 5, 'quim': 5, 'pomegranate': 5, 'sorehead': 5, 'woker': 5, 'beaner': 4, 'peeler': 4, 'bleeder': 4, 'bell ringer': 4, 'knocker': 4, 'tonk': 4, 'khazar': 4, 'middlebrow': 4, 'baby face': 4, 'romanist': 4, 'subnormal': 4, 'tryhard': 4, 'westie': 3, 'dickies': 3, 'moll': 3, 'bumf': 3, 'chrome dome': 3, 'mugwump': 3, 'rain man': 3, 'bolshie': 3, 'wifie': 3, 'aborter': 3, 'one-man band': 3, 'half-breed': 3, 'pelf': 3, 'slattern': 3, 'streetwalker': 3, 'dip stick': 3, 'blackwashing': 3, 'blue-eyed boy': 3, 'asur': 3, 'snakehead': 2, 'kinglet': 2, 'butterball': 2, 'black gangster': 2, 'knacker': 2, 'termagant': 2, 'foofoo': 2, 'indian time': 2, 'jelly roll': 2, 'minute man': 2, 'frit': 2, 'clanker': 2, 'pea patch': 2, 'banderite': 2, 'buttinsky': 2, 'mucker': 2, 'flea bag': 2, 'scullion': 2, 'mong': 2, 'fathead': 2, 'brach': 2, 'peeper': 2, 'smearer': 2, 'nicker': 2, 'moggy': 2, 'wobbler': 2, 'scally': 2, 'senga': 1, 'bashaw': 1, 'dog robber': 1, 'chinky': 1, 'scummer': 1, 'blueshirt': 1, 'carbage': 1, 'time vampire': 1, 'dopper': 1, 'epicene': 1, 'keeno': 1, 'scallywag': 1, 'rockhead': 1, 'dogface': 1, 'obeast': 1, 'flunkey': 1, 'white monkey': 1, 'pisher': 1, 'chickenhead': 1, \"wrong 'un\": 1, 'awokening': 1, 'munter': 1, 'scalie': 1, 'golliwog': 1}\n",
    "# Metodo 2: Usando filter\n",
    "chiavi = filter(lambda k: dizionario[k] < 2, dizionario.keys())\n",
    "chiavi = set([c.lower() for c in chiavi])\n",
    "len(chiavi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cb0bb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\apriori_lexicon.json', 'r') as f:\n",
    "    lexicon = json.load(f)\n",
    "\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86824d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keys = set([w.lower() for w in dizionario.keys()])\n",
    "len(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c745dd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = set([word['word'].lower() for word in lexicon if word['word'].lower() not in all_keys])\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44dcf9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_keys.intersection(chiavi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39d3a13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_be_removed = missing.union(chiavi)\n",
    "len(to_be_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "017fef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_words = set(word['word'].lower() for word in lexicon)\n",
    "len(lexicon_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48cc5ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lexicon = [word for word in lexicon if word['word'].lower() not in to_be_removed]\n",
    "len(new_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0507a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\apriori_lexicon.json', 'w') as f:\n",
    "    json.dump(new_lexicon, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d860a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = dataset['word']\n",
    "counts = Counter(words)\n",
    "\n",
    "zero_count_words = [word for word in lexicon if counts.get(word, 0) == 0]\n",
    "\n",
    "result = {word: counts.get(word, 0) for word in lexicon}\n",
    "sorted_result = dict(sorted(result.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Stampa in ordine decrescente\n",
    "for word, count in sorted_result.items():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb6e1fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wokery', 'greenshirt', 'jehovist', 'bourgie', 'musk cat', 'queenlet', 'guppie', 'walloper', 'paddyism', 'gippo', 'bogger', 'frigger', 'hoopy', 'ham-and-egger', 'lime-juicer', 'fleshpot', 'pommy', 'turnspit', 'holidayism', 'grass-eater', 'gobby', 'rhodie', 'red nigger', 'magus', 'paskudnyak', 'milk drinker', 'shaveling', 'fagdom', 'yiddo', 'kugel', 'rice king', 'hindoo', 'gallicanism', 'social imperialism', 'whipstitch', 'hindian', 'mud shark', 'pickaninny', 'dobber', 'yam yam', 'new chum', 'noodler', 'golden ear', 'paradoxist', 'besom', 'broadbrim', 'roider', 'crossback', 'tutti-frutti', 'booner', 'chugger', 'hagiographer', 'niggerhead', 'fleshling', 'uniform queen', 'mameluke', 'scrote', 'repper', 'chevisance', 'whoremaster', 'pot boiler', 'cornfed', 'sprog', 'raggie', 'box checker', 'marmoset', 'corn-cracker', 'fatface', 'clitty', 'unteacher', 'painted jezebel', 'pickaninny christmas', 'lardy', 'zip coon', 'radge', 'bibler', 'nitchie', 'bluestocking', 'chanate', 'dudelet', 'bristler', 'charva', 'sheeny', 'bolitics', 'chirper', 'womanish', 'hoojah', 'tarrier', 'whoreson', 'foamer', 'bogtrotter', 'no-life', 'speccy', 'franquist', 'ploppy', 'beaver eater', 'creeker', 'flatfoot', 'mombie', 'cheese bus', 'bummery', 'bantling', 'schafskopf', 'cruff', 'paddywhack', 'pussydom', 'body snatcher', 'clart', 'junkball', 'nipcheese', 'free-willer', 'gegger', 'dappa', 'cheeser', 'bludger', 'squit', 'nobber', 'bamp', 'preciosity', 'toe rag', 'welshy', 'chinese home run', 'buckra', 'goback', 'scrag', 'bluey', 'mackem', 'musclebound', 'gutter dog']\n"
     ]
    }
   ],
   "source": [
    "print(zero_count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be474d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\lexicon.json', 'r') as f:\n",
    "    full_lexicon = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28075b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = [entry for entry in full_lexicon if entry['word'].lower() not in zero_count_words]\n",
    "with open('C:\\\\Users\\\\Dario\\\\Desktop\\\\Dario\\\\Uni\\\\Tirocinio\\\\LSS-in-Hate-Speech-Detection\\\\Lexicon\\\\my_lexicon\\\\LND_lexicon.json', 'w') as f:\n",
    "    json.dump(full_lexicon, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c3e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.encode('mi piacciono i treni')\n",
    "b = model.encode('mi piacciono i treni', convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "623c52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool=model.start_multi_process_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad2089ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.encode_multi_process(['mi piacciono i treni', 'i treni sono belli'], pool=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a063ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': ['2017', '2017'],\n",
       " 'text': ['I really wish the DP would bounce people trying to make a moral equivalence between Nazis and BLM. \\n\\n\\nThat kind of hateful BothSiderist garbage had got to stop. It\\'s not even funny any more. \\n\\n\\nActual Nazis are marching in this country and killing people. And the response of some is \"but the blahs\". And the DP allows such posts to stay up, and the posters to continue posting. \\n\\n\\nUgly.',\n",
       "  'I really wish the DP would bounce people trying to make a moral equivalence between Nazis and BLM. \\n\\n\\nThat kind of hateful BothSiderist garbage had got to stop. It\\'s not even funny any more. \\n\\n\\nActual Nazis are marching in this country and killing people. And the response of some is \"but the blahs\". And the DP allows such posts to stay up, and the posters to continue posting. \\n\\n\\nUgly.'],\n",
       " 'word': ['funny', 'ugly']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbcc23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
