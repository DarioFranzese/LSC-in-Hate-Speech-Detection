{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35aba6d",
   "metadata": {},
   "source": [
    "# Offensive Words Collection from Urban Dictionary\n",
    "\n",
    "This notebook collects offensive words from Urban Dictionary, organized by year (2005-2025), with WordNet validation and offensive content scoring.\n",
    "\n",
    "**Features:**\n",
    "- Chronological organization by first definition date\n",
    "- WordNet validation (requires multiple synsets)\n",
    "- Offensive content scoring system\n",
    "- Configurable total words and words per year\n",
    "- JSON export functionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24825608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b2c05",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define keywords and patterns used for offensive content detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5ec21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  - 19 offensive keywords\n",
      "  - 7 explicit words\n",
      "  - 4 person contexts\n"
     ]
    }
   ],
   "source": [
    "OFFENSIVE_KEYWORDS = [\n",
    "    'insult', 'offensive', 'derogatory', 'rude', 'hateful', 'abusive',\n",
    "    'contemptuous', 'pejorative', 'slur', 'racist', 'sexist', 'bigot',\n",
    "    'discriminat', 'harass', 'mock', 'demean', 'stupid', 'idiot', 'loser'\n",
    "]\n",
    "\n",
    "EXPLICIT_WORDS = ['fuck', 'shit', 'ass', 'hell', 'dick', 'bitch', 'bastard']\n",
    "PERSON_CONTEXTS = ['someone who', 'person who', 'people who', 'anyone who']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5e566",
   "metadata": {},
   "source": [
    "## API Functions\n",
    "\n",
    "Functions to interact with Urban Dictionary API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API connection successful - Retrieved 10 random entries\n"
     ]
    }
   ],
   "source": [
    "def get_random_urban_words():\n",
    "    \"\"\"Fetch random words from Urban Dictionary API\"\"\"\n",
    "    url = \"https://api.urbandictionary.com/v0/random\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get('list', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching random words: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_word_definitions(word):\n",
    "    \"\"\"Fetch all definitions for a specific word\"\"\"\n",
    "    url = \"https://api.urbandictionary.com/v0/define\"\n",
    "    try:\n",
    "        response = requests.get(url, params={'term': word}, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get('list', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching '{word}': {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8771c95f",
   "metadata": {},
   "source": [
    "## WordNet Validation\n",
    "\n",
    "Validate words using WordNet to ensure they have multiple synsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5421047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet validation test:\n",
      "  dog        → 8 synsets → ✓ Valid\n",
      "  run        → 57 synsets → ✓ Valid\n",
      "  asdfgh     → 0 synsets → ✗ Invalid\n",
      "  test       → 13 synsets → ✓ Valid\n"
     ]
    }
   ],
   "source": [
    "def has_multiple_synsets(word):\n",
    "    \"\"\"\n",
    "    Check if a word has more than one synset in WordNet.\n",
    "    Words with multiple synsets might have semantically shifted in time.\n",
    "    \"\"\"\n",
    "    word_clean = word.lower().replace(' ', '_')\n",
    "    synsets = wordnet.synsets(word_clean)\n",
    "    return len(synsets) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc029b",
   "metadata": {},
   "source": [
    "## Offensive Content Analysis\n",
    "\n",
    "Analyze text content to identify and score offensive language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48cb17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offensive scoring test:\n",
      "  Definition 1: score = 1\n",
      "  Definition 2: score = 5\n"
     ]
    }
   ],
   "source": [
    "def is_offensive_text(text):\n",
    "    \"\"\"Check if text contains offensive language\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in OFFENSIVE_KEYWORDS)\n",
    "\n",
    "def calculate_offensive_score(definitions):\n",
    "    \"\"\"\n",
    "    Calculate offensive score based on definitions and examples.\n",
    "    Higher score = more likely to be offensive content.\n",
    "    \n",
    "    Scoring:\n",
    "    - Offensive keywords in definition: +3\n",
    "    - Offensive keywords in example: +2\n",
    "    - Explicit words: +2\n",
    "    - Person contexts: +1\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    for entry in definitions:\n",
    "        definition = entry.get('definition', '').lower()\n",
    "        example = entry.get('example', '').lower()\n",
    "        \n",
    "        # Check for offensive keywords\n",
    "        if is_offensive_text(definition):\n",
    "            score += 3\n",
    "        if is_offensive_text(example):\n",
    "            score += 2\n",
    "        \n",
    "        # Check for explicit words\n",
    "        if any(word in definition or word in example for word in EXPLICIT_WORDS):\n",
    "            score += 2\n",
    "        \n",
    "        # Check for negative person contexts\n",
    "        if any(ctx in definition for ctx in PERSON_CONTEXTS):\n",
    "            score += 1\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d7166",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Extract and process word data from Urban Dictionary entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fabf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data processing functions loaded\n"
     ]
    }
   ],
   "source": [
    "def extract_year(date_string):\n",
    "    \"\"\"Extract year from ISO date string\"\"\"\n",
    "    try:\n",
    "        date_obj = datetime.fromisoformat(date_string.replace('Z', '+00:00'))\n",
    "        return date_obj.year\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove Urban Dictionary special characters\"\"\"\n",
    "    return text.replace('[', '').replace(']', '').strip()\n",
    "\n",
    "def process_word(word_entries, min_score=4):\n",
    "    \"\"\"\n",
    "    Process word entries and return aggregated data.\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys: word, year, definition, examples\n",
    "        None if word doesn't meet criteria\n",
    "    \"\"\"\n",
    "    if not word_entries:\n",
    "        return None\n",
    "    \n",
    "    # Sort by date (oldest first)\n",
    "    sorted_entries = sorted(word_entries, key=lambda x: x.get('written_on', ''))\n",
    "    \n",
    "    # Find first valid year\n",
    "    first_year = None\n",
    "    for entry in sorted_entries:\n",
    "        year = extract_year(entry.get('written_on', ''))\n",
    "        if year and 2005 <= year <= 2025:\n",
    "            first_year = year\n",
    "            break\n",
    "    \n",
    "    if not first_year:\n",
    "        return None\n",
    "    \n",
    "    # Check offensive score\n",
    "    score = calculate_offensive_score(word_entries)\n",
    "    if score < min_score:\n",
    "        return None\n",
    "    \n",
    "    # Validate with WordNet\n",
    "    word = word_entries[0]['word']\n",
    "    if not has_multiple_synsets(word):\n",
    "        return None\n",
    "    \n",
    "    # Aggregate definitions and examples (max 5)\n",
    "    definitions = []\n",
    "    examples = []\n",
    "    \n",
    "    for entry in word_entries[:5]:\n",
    "        definition = clean_text(entry.get('definition', ''))\n",
    "        example = clean_text(entry.get('example', ''))\n",
    "        \n",
    "        if definition and definition not in definitions:\n",
    "            definitions.append(definition)\n",
    "        if example and example not in examples:\n",
    "            examples.append(example)\n",
    "    \n",
    "    return {\n",
    "        'word': word,\n",
    "        'year': first_year,\n",
    "        'definition': ' | '.join(definitions),\n",
    "        'examples': ' | '.join(examples)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdf417",
   "metadata": {},
   "source": [
    "## Main Collection Function\n",
    "\n",
    "Core function to collect offensive words organized by year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collection function ready\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def collect_offensive_words(total_words=100, words_per_year=5):\n",
    "    \"\"\"\n",
    "    Collect offensive words organized by year.\n",
    "    \n",
    "    Args:\n",
    "        total_words (int): Total number of words to collect\n",
    "        words_per_year (int): Target words per year (2005-2025)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Words organized by year\n",
    "    \"\"\"\n",
    "    years = list(range(2005, 2026))  # 2005-2025\n",
    "    \n",
    "    if words_per_year is None:\n",
    "        words_per_year = total_words // len(years)\n",
    "    \n",
    "    words_by_year = defaultdict(list)\n",
    "    seen_words = set()\n",
    "    attempts = 0\n",
    "    max_attempts = total_words * 50  # Safety limit\n",
    "    \n",
    "    print(f\"🎯 Target: {total_words} total words ({words_per_year} per year)\")\n",
    "    print(f\"📅 Period: 2005-2025\")\n",
    "    print(f\"🔍 Filters: WordNet (>1 synset) + offensive score\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(total=total_words, desc=\"Collecting words\", unit=\"word\")\n",
    "    \n",
    "    while attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        \n",
    "        # Get batch of random words\n",
    "        entries = get_random_urban_words()\n",
    "        if not entries:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        \n",
    "        for entry in entries:\n",
    "            word = entry.get('word', '').lower().strip()\n",
    "            \n",
    "            # Skip if already processed\n",
    "            if not word or word in seen_words:\n",
    "                continue\n",
    "            \n",
    "            # Get all definitions\n",
    "            all_defs = get_word_definitions(word)\n",
    "            time.sleep(0.3)  # Rate limiting\n",
    "            \n",
    "            if not all_defs:\n",
    "                continue\n",
    "            \n",
    "            # Process the word\n",
    "            word_data = process_word(all_defs)\n",
    "            \n",
    "            if word_data:\n",
    "                year = word_data['year']\n",
    "                \n",
    "                # Add if still needed for that year\n",
    "                if len(words_by_year[year]) < words_per_year:\n",
    "                    words_by_year[year].append(word_data)\n",
    "                    seen_words.add(word)\n",
    "                    \n",
    "                    total_collected = sum(len(v) for v in words_by_year.values())\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix({'word': word[:15], 'year': year})\n",
    "                    \n",
    "                    # Check completion\n",
    "                    if total_collected >= total_words:\n",
    "                        pbar.close()\n",
    "                        print(\"\\n✅ Collection completed!\")\n",
    "                        return words_by_year\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    pbar.close()\n",
    "    print(f\"\\n⚠️ Reached limit of {max_attempts} attempts\")\n",
    "    return words_by_year\n",
    "\n",
    "print(\"✓ Collection function ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d20991",
   "metadata": {},
   "source": [
    "## Output and Export Functions\n",
    "\n",
    "Save results and display summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46709543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Output functions ready\n"
     ]
    }
   ],
   "source": [
    "def save_results(words_by_year, filename='offensive_words.json'):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    results = []\n",
    "    for year in sorted(words_by_year.keys()):\n",
    "        results.extend(words_by_year[year])\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n💾 Saved: {filename}\")\n",
    "    return results\n",
    "\n",
    "def print_summary(words_by_year):\n",
    "    \"\"\"Print collection summary\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"📋 SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for year in sorted(words_by_year.keys()):\n",
    "        words = words_by_year[year]\n",
    "        print(f\"\\n{year} → {len(words)} words:\")\n",
    "        for w in words:\n",
    "            defn = w['definition'][:50] + \"...\" if len(w['definition']) > 50 else w['definition']\n",
    "            print(f\"  • {w['word']:20} {defn}\")\n",
    "    \n",
    "    total = sum(len(v) for v in words_by_year.values())\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"TOTAL: {total} words across {len(words_by_year)} years\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "print(\"✓ Output functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c95a2e",
   "metadata": {},
   "source": [
    "## Run Collection\n",
    "\n",
    "Execute the collection process with configurable parameters.\n",
    "\n",
    "**Parameters:**\n",
    "- `TOTAL_WORDS`: Total number of offensive words to collect\n",
    "- `WORDS_PER_YEAR`: Target number of words per year (2005-2025)\n",
    "\n",
    "⚠️ **Warning:** This process may take 30-60 minutes depending on the target number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURABLE PARAMETERS\n",
    "TOTAL_WORDS = 20      # Change this for different total\n",
    "WORDS_PER_YEAR = 1     # Change this for words per year\n",
    "\n",
    "# Run collection\n",
    "words_by_year = collect_offensive_words(\n",
    "    total_words=TOTAL_WORDS,\n",
    "    words_per_year=WORDS_PER_YEAR\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results = save_results(words_by_year, filename='offensive_words.json')\n",
    "\n",
    "# Display summary\n",
    "print_summary(words_by_year)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
